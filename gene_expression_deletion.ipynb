{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15f5ff67-0ddc-43c6-b48a-2941be338093",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "CUDA extension not installed.\n",
      "CUDA extension not installed.\n",
      "`loss_type=None` was set in the config but it is unrecognised.Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e1bf8ca093a43c6acb0aa680523274a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/11 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at /mnt/zf/Qwen1.5-72B-Chat-GPTQ-Int4 were not used when initializing Qwen2ForCausalLM: ['model.layers.0.mlp.down_proj.bias', 'model.layers.0.mlp.gate_proj.bias', 'model.layers.0.mlp.up_proj.bias', 'model.layers.0.self_attn.o_proj.bias', 'model.layers.1.mlp.down_proj.bias', 'model.layers.1.mlp.gate_proj.bias', 'model.layers.1.mlp.up_proj.bias', 'model.layers.1.self_attn.o_proj.bias', 'model.layers.10.mlp.down_proj.bias', 'model.layers.10.mlp.gate_proj.bias', 'model.layers.10.mlp.up_proj.bias', 'model.layers.10.self_attn.o_proj.bias', 'model.layers.11.mlp.down_proj.bias', 'model.layers.11.mlp.gate_proj.bias', 'model.layers.11.mlp.up_proj.bias', 'model.layers.11.self_attn.o_proj.bias', 'model.layers.12.mlp.down_proj.bias', 'model.layers.12.mlp.gate_proj.bias', 'model.layers.12.mlp.up_proj.bias', 'model.layers.12.self_attn.o_proj.bias', 'model.layers.13.mlp.down_proj.bias', 'model.layers.13.mlp.gate_proj.bias', 'model.layers.13.mlp.up_proj.bias', 'model.layers.13.self_attn.o_proj.bias', 'model.layers.14.mlp.down_proj.bias', 'model.layers.14.mlp.gate_proj.bias', 'model.layers.14.mlp.up_proj.bias', 'model.layers.14.self_attn.o_proj.bias', 'model.layers.15.mlp.down_proj.bias', 'model.layers.15.mlp.gate_proj.bias', 'model.layers.15.mlp.up_proj.bias', 'model.layers.15.self_attn.o_proj.bias', 'model.layers.16.mlp.down_proj.bias', 'model.layers.16.mlp.gate_proj.bias', 'model.layers.16.mlp.up_proj.bias', 'model.layers.16.self_attn.o_proj.bias', 'model.layers.17.mlp.down_proj.bias', 'model.layers.17.mlp.gate_proj.bias', 'model.layers.17.mlp.up_proj.bias', 'model.layers.17.self_attn.o_proj.bias', 'model.layers.18.mlp.down_proj.bias', 'model.layers.18.mlp.gate_proj.bias', 'model.layers.18.mlp.up_proj.bias', 'model.layers.18.self_attn.o_proj.bias', 'model.layers.19.mlp.down_proj.bias', 'model.layers.19.mlp.gate_proj.bias', 'model.layers.19.mlp.up_proj.bias', 'model.layers.19.self_attn.o_proj.bias', 'model.layers.2.mlp.down_proj.bias', 'model.layers.2.mlp.gate_proj.bias', 'model.layers.2.mlp.up_proj.bias', 'model.layers.2.self_attn.o_proj.bias', 'model.layers.20.mlp.down_proj.bias', 'model.layers.20.mlp.gate_proj.bias', 'model.layers.20.mlp.up_proj.bias', 'model.layers.20.self_attn.o_proj.bias', 'model.layers.21.mlp.down_proj.bias', 'model.layers.21.mlp.gate_proj.bias', 'model.layers.21.mlp.up_proj.bias', 'model.layers.21.self_attn.o_proj.bias', 'model.layers.22.mlp.down_proj.bias', 'model.layers.22.mlp.gate_proj.bias', 'model.layers.22.mlp.up_proj.bias', 'model.layers.22.self_attn.o_proj.bias', 'model.layers.23.mlp.down_proj.bias', 'model.layers.23.mlp.gate_proj.bias', 'model.layers.23.mlp.up_proj.bias', 'model.layers.23.self_attn.o_proj.bias', 'model.layers.24.mlp.down_proj.bias', 'model.layers.24.mlp.gate_proj.bias', 'model.layers.24.mlp.up_proj.bias', 'model.layers.24.self_attn.o_proj.bias', 'model.layers.25.mlp.down_proj.bias', 'model.layers.25.mlp.gate_proj.bias', 'model.layers.25.mlp.up_proj.bias', 'model.layers.25.self_attn.o_proj.bias', 'model.layers.26.mlp.down_proj.bias', 'model.layers.26.mlp.gate_proj.bias', 'model.layers.26.mlp.up_proj.bias', 'model.layers.26.self_attn.o_proj.bias', 'model.layers.27.mlp.down_proj.bias', 'model.layers.27.mlp.gate_proj.bias', 'model.layers.27.mlp.up_proj.bias', 'model.layers.27.self_attn.o_proj.bias', 'model.layers.28.mlp.down_proj.bias', 'model.layers.28.mlp.gate_proj.bias', 'model.layers.28.mlp.up_proj.bias', 'model.layers.28.self_attn.o_proj.bias', 'model.layers.29.mlp.down_proj.bias', 'model.layers.29.mlp.gate_proj.bias', 'model.layers.29.mlp.up_proj.bias', 'model.layers.29.self_attn.o_proj.bias', 'model.layers.3.mlp.down_proj.bias', 'model.layers.3.mlp.gate_proj.bias', 'model.layers.3.mlp.up_proj.bias', 'model.layers.3.self_attn.o_proj.bias', 'model.layers.30.mlp.down_proj.bias', 'model.layers.30.mlp.gate_proj.bias', 'model.layers.30.mlp.up_proj.bias', 'model.layers.30.self_attn.o_proj.bias', 'model.layers.31.mlp.down_proj.bias', 'model.layers.31.mlp.gate_proj.bias', 'model.layers.31.mlp.up_proj.bias', 'model.layers.31.self_attn.o_proj.bias', 'model.layers.32.mlp.down_proj.bias', 'model.layers.32.mlp.gate_proj.bias', 'model.layers.32.mlp.up_proj.bias', 'model.layers.32.self_attn.o_proj.bias', 'model.layers.33.mlp.down_proj.bias', 'model.layers.33.mlp.gate_proj.bias', 'model.layers.33.mlp.up_proj.bias', 'model.layers.33.self_attn.o_proj.bias', 'model.layers.34.mlp.down_proj.bias', 'model.layers.34.mlp.gate_proj.bias', 'model.layers.34.mlp.up_proj.bias', 'model.layers.34.self_attn.o_proj.bias', 'model.layers.35.mlp.down_proj.bias', 'model.layers.35.mlp.gate_proj.bias', 'model.layers.35.mlp.up_proj.bias', 'model.layers.35.self_attn.o_proj.bias', 'model.layers.36.mlp.down_proj.bias', 'model.layers.36.mlp.gate_proj.bias', 'model.layers.36.mlp.up_proj.bias', 'model.layers.36.self_attn.o_proj.bias', 'model.layers.37.mlp.down_proj.bias', 'model.layers.37.mlp.gate_proj.bias', 'model.layers.37.mlp.up_proj.bias', 'model.layers.37.self_attn.o_proj.bias', 'model.layers.38.mlp.down_proj.bias', 'model.layers.38.mlp.gate_proj.bias', 'model.layers.38.mlp.up_proj.bias', 'model.layers.38.self_attn.o_proj.bias', 'model.layers.39.mlp.down_proj.bias', 'model.layers.39.mlp.gate_proj.bias', 'model.layers.39.mlp.up_proj.bias', 'model.layers.39.self_attn.o_proj.bias', 'model.layers.4.mlp.down_proj.bias', 'model.layers.4.mlp.gate_proj.bias', 'model.layers.4.mlp.up_proj.bias', 'model.layers.4.self_attn.o_proj.bias', 'model.layers.40.mlp.down_proj.bias', 'model.layers.40.mlp.gate_proj.bias', 'model.layers.40.mlp.up_proj.bias', 'model.layers.40.self_attn.o_proj.bias', 'model.layers.41.mlp.down_proj.bias', 'model.layers.41.mlp.gate_proj.bias', 'model.layers.41.mlp.up_proj.bias', 'model.layers.41.self_attn.o_proj.bias', 'model.layers.42.mlp.down_proj.bias', 'model.layers.42.mlp.gate_proj.bias', 'model.layers.42.mlp.up_proj.bias', 'model.layers.42.self_attn.o_proj.bias', 'model.layers.43.mlp.down_proj.bias', 'model.layers.43.mlp.gate_proj.bias', 'model.layers.43.mlp.up_proj.bias', 'model.layers.43.self_attn.o_proj.bias', 'model.layers.44.mlp.down_proj.bias', 'model.layers.44.mlp.gate_proj.bias', 'model.layers.44.mlp.up_proj.bias', 'model.layers.44.self_attn.o_proj.bias', 'model.layers.45.mlp.down_proj.bias', 'model.layers.45.mlp.gate_proj.bias', 'model.layers.45.mlp.up_proj.bias', 'model.layers.45.self_attn.o_proj.bias', 'model.layers.46.mlp.down_proj.bias', 'model.layers.46.mlp.gate_proj.bias', 'model.layers.46.mlp.up_proj.bias', 'model.layers.46.self_attn.o_proj.bias', 'model.layers.47.mlp.down_proj.bias', 'model.layers.47.mlp.gate_proj.bias', 'model.layers.47.mlp.up_proj.bias', 'model.layers.47.self_attn.o_proj.bias', 'model.layers.48.mlp.down_proj.bias', 'model.layers.48.mlp.gate_proj.bias', 'model.layers.48.mlp.up_proj.bias', 'model.layers.48.self_attn.o_proj.bias', 'model.layers.49.mlp.down_proj.bias', 'model.layers.49.mlp.gate_proj.bias', 'model.layers.49.mlp.up_proj.bias', 'model.layers.49.self_attn.o_proj.bias', 'model.layers.5.mlp.down_proj.bias', 'model.layers.5.mlp.gate_proj.bias', 'model.layers.5.mlp.up_proj.bias', 'model.layers.5.self_attn.o_proj.bias', 'model.layers.50.mlp.down_proj.bias', 'model.layers.50.mlp.gate_proj.bias', 'model.layers.50.mlp.up_proj.bias', 'model.layers.50.self_attn.o_proj.bias', 'model.layers.51.mlp.down_proj.bias', 'model.layers.51.mlp.gate_proj.bias', 'model.layers.51.mlp.up_proj.bias', 'model.layers.51.self_attn.o_proj.bias', 'model.layers.52.mlp.down_proj.bias', 'model.layers.52.mlp.gate_proj.bias', 'model.layers.52.mlp.up_proj.bias', 'model.layers.52.self_attn.o_proj.bias', 'model.layers.53.mlp.down_proj.bias', 'model.layers.53.mlp.gate_proj.bias', 'model.layers.53.mlp.up_proj.bias', 'model.layers.53.self_attn.o_proj.bias', 'model.layers.54.mlp.down_proj.bias', 'model.layers.54.mlp.gate_proj.bias', 'model.layers.54.mlp.up_proj.bias', 'model.layers.54.self_attn.o_proj.bias', 'model.layers.55.mlp.down_proj.bias', 'model.layers.55.mlp.gate_proj.bias', 'model.layers.55.mlp.up_proj.bias', 'model.layers.55.self_attn.o_proj.bias', 'model.layers.56.mlp.down_proj.bias', 'model.layers.56.mlp.gate_proj.bias', 'model.layers.56.mlp.up_proj.bias', 'model.layers.56.self_attn.o_proj.bias', 'model.layers.57.mlp.down_proj.bias', 'model.layers.57.mlp.gate_proj.bias', 'model.layers.57.mlp.up_proj.bias', 'model.layers.57.self_attn.o_proj.bias', 'model.layers.58.mlp.down_proj.bias', 'model.layers.58.mlp.gate_proj.bias', 'model.layers.58.mlp.up_proj.bias', 'model.layers.58.self_attn.o_proj.bias', 'model.layers.59.mlp.down_proj.bias', 'model.layers.59.mlp.gate_proj.bias', 'model.layers.59.mlp.up_proj.bias', 'model.layers.59.self_attn.o_proj.bias', 'model.layers.6.mlp.down_proj.bias', 'model.layers.6.mlp.gate_proj.bias', 'model.layers.6.mlp.up_proj.bias', 'model.layers.6.self_attn.o_proj.bias', 'model.layers.60.mlp.down_proj.bias', 'model.layers.60.mlp.gate_proj.bias', 'model.layers.60.mlp.up_proj.bias', 'model.layers.60.self_attn.o_proj.bias', 'model.layers.61.mlp.down_proj.bias', 'model.layers.61.mlp.gate_proj.bias', 'model.layers.61.mlp.up_proj.bias', 'model.layers.61.self_attn.o_proj.bias', 'model.layers.62.mlp.down_proj.bias', 'model.layers.62.mlp.gate_proj.bias', 'model.layers.62.mlp.up_proj.bias', 'model.layers.62.self_attn.o_proj.bias', 'model.layers.63.mlp.down_proj.bias', 'model.layers.63.mlp.gate_proj.bias', 'model.layers.63.mlp.up_proj.bias', 'model.layers.63.self_attn.o_proj.bias', 'model.layers.64.mlp.down_proj.bias', 'model.layers.64.mlp.gate_proj.bias', 'model.layers.64.mlp.up_proj.bias', 'model.layers.64.self_attn.o_proj.bias', 'model.layers.65.mlp.down_proj.bias', 'model.layers.65.mlp.gate_proj.bias', 'model.layers.65.mlp.up_proj.bias', 'model.layers.65.self_attn.o_proj.bias', 'model.layers.66.mlp.down_proj.bias', 'model.layers.66.mlp.gate_proj.bias', 'model.layers.66.mlp.up_proj.bias', 'model.layers.66.self_attn.o_proj.bias', 'model.layers.67.mlp.down_proj.bias', 'model.layers.67.mlp.gate_proj.bias', 'model.layers.67.mlp.up_proj.bias', 'model.layers.67.self_attn.o_proj.bias', 'model.layers.68.mlp.down_proj.bias', 'model.layers.68.mlp.gate_proj.bias', 'model.layers.68.mlp.up_proj.bias', 'model.layers.68.self_attn.o_proj.bias', 'model.layers.69.mlp.down_proj.bias', 'model.layers.69.mlp.gate_proj.bias', 'model.layers.69.mlp.up_proj.bias', 'model.layers.69.self_attn.o_proj.bias', 'model.layers.7.mlp.down_proj.bias', 'model.layers.7.mlp.gate_proj.bias', 'model.layers.7.mlp.up_proj.bias', 'model.layers.7.self_attn.o_proj.bias', 'model.layers.70.mlp.down_proj.bias', 'model.layers.70.mlp.gate_proj.bias', 'model.layers.70.mlp.up_proj.bias', 'model.layers.70.self_attn.o_proj.bias', 'model.layers.71.mlp.down_proj.bias', 'model.layers.71.mlp.gate_proj.bias', 'model.layers.71.mlp.up_proj.bias', 'model.layers.71.self_attn.o_proj.bias', 'model.layers.72.mlp.down_proj.bias', 'model.layers.72.mlp.gate_proj.bias', 'model.layers.72.mlp.up_proj.bias', 'model.layers.72.self_attn.o_proj.bias', 'model.layers.73.mlp.down_proj.bias', 'model.layers.73.mlp.gate_proj.bias', 'model.layers.73.mlp.up_proj.bias', 'model.layers.73.self_attn.o_proj.bias', 'model.layers.74.mlp.down_proj.bias', 'model.layers.74.mlp.gate_proj.bias', 'model.layers.74.mlp.up_proj.bias', 'model.layers.74.self_attn.o_proj.bias', 'model.layers.75.mlp.down_proj.bias', 'model.layers.75.mlp.gate_proj.bias', 'model.layers.75.mlp.up_proj.bias', 'model.layers.75.self_attn.o_proj.bias', 'model.layers.76.mlp.down_proj.bias', 'model.layers.76.mlp.gate_proj.bias', 'model.layers.76.mlp.up_proj.bias', 'model.layers.76.self_attn.o_proj.bias', 'model.layers.77.mlp.down_proj.bias', 'model.layers.77.mlp.gate_proj.bias', 'model.layers.77.mlp.up_proj.bias', 'model.layers.77.self_attn.o_proj.bias', 'model.layers.78.mlp.down_proj.bias', 'model.layers.78.mlp.gate_proj.bias', 'model.layers.78.mlp.up_proj.bias', 'model.layers.78.self_attn.o_proj.bias', 'model.layers.79.mlp.down_proj.bias', 'model.layers.79.mlp.gate_proj.bias', 'model.layers.79.mlp.up_proj.bias', 'model.layers.79.self_attn.o_proj.bias', 'model.layers.8.mlp.down_proj.bias', 'model.layers.8.mlp.gate_proj.bias', 'model.layers.8.mlp.up_proj.bias', 'model.layers.8.self_attn.o_proj.bias', 'model.layers.9.mlp.down_proj.bias', 'model.layers.9.mlp.gate_proj.bias', 'model.layers.9.mlp.up_proj.bias', 'model.layers.9.self_attn.o_proj.bias']\n",
      "- This IS expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing Qwen2ForCausalLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"USE_FLASH_ATTENTION\"] = \"1\"\n",
    "\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "device = \"cuda\" # the device to load the model onto\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(r\"/mnt/zf/Qwen1.5-72B-Chat-GPTQ-Int4\",\n",
    "    device_map=\"auto\", torch_dtype=\"auto\",attn_implementation=\"flash_attention_2\",\n",
    ")\n",
    "tokenizer = AutoTokenizer.from_pretrained(r\"/mnt/zf/Qwen1.5-72B-Chat-GPTQ-Int4\")\n",
    "\n",
    "#Qwen1.5-72B-Chat-GPTQ-Int4\n",
    "#Qwen1.5-14B-Chat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "34f38374-11ac-4236-8b83-7e19083eac9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved combined search results to Bacillus subtilis lipopeptide or surfactin biosynthetic gene expressionBacillus subtilis lipopeptide or surfactin biosynthetic gene deletion_pubmed_search_results.xlsx\n"
     ]
    }
   ],
   "source": [
    "from Bio import Entrez\n",
    "import pandas as pd\n",
    "\n",
    "# Define your email to use with NCBI Entrez\n",
    "Entrez.email = \"xiao.zhengyang@wustl.edu\"\n",
    "\n",
    "def search_pubmed(keyword):\n",
    "    # Adjust the search term to focus on abstracts\n",
    "    search_term = f\"{keyword}[Abstract]\"\n",
    "    handle = Entrez.esearch(db=\"pubmed\", term=search_term, retmax=1000)\n",
    "    record = Entrez.read(handle)\n",
    "    handle.close()\n",
    "    # Get the list of Ids returned by the search\n",
    "    id_list = record[\"IdList\"]\n",
    "    return id_list\n",
    "\n",
    "def fetch_details(id_list):\n",
    "    ids = ','.join(id_list)\n",
    "    handle = Entrez.efetch(db=\"pubmed\", id=ids, retmode=\"xml\")\n",
    "    records = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    # Create a list to hold our article details\n",
    "    articles = []\n",
    "\n",
    "    for pubmed_article in records['PubmedArticle']:\n",
    "        article = {}\n",
    "        article_data = pubmed_article['MedlineCitation']['Article']\n",
    "        article['Title'] = article_data.get('ArticleTitle')\n",
    "        \n",
    "        # Directly output the abstract\n",
    "        abstract_text = article_data.get('Abstract', {}).get('AbstractText', [])\n",
    "        if isinstance(abstract_text, list):\n",
    "            abstract_text = ' '.join(abstract_text)\n",
    "        article['Abstract'] = abstract_text\n",
    "\n",
    "        article['Journal'] = article_data.get('Journal', {}).get('Title')\n",
    "\n",
    "        articles.append(article)\n",
    "\n",
    "    return articles\n",
    "\n",
    "def perform_search_and_fetch(keyword):\n",
    "    id_list = search_pubmed(keyword)\n",
    "    return fetch_details(id_list)\n",
    "\n",
    "# Example usage: Performing two searches\n",
    "keyword1 = \"Bacillus subtilis lipopeptide or surfactin biosynthetic gene expression\"\n",
    "keyword2 = \"Bacillus subtilis lipopeptide or surfactin biosynthetic gene deletion\"\n",
    "keyword = keyword1 + keyword2\n",
    "# Fetch articles for both keywords\n",
    "articles1 = perform_search_and_fetch(keyword1)\n",
    "articles2 = perform_search_and_fetch(keyword2)\n",
    "\n",
    "# Convert both lists of articles to DataFrames\n",
    "df1 = pd.DataFrame(articles1)\n",
    "df2 = pd.DataFrame(articles2)\n",
    "\n",
    "# Add a column to differentiate the search terms in the final DataFrame\n",
    "df1['SearchTerm'] = keyword1\n",
    "df2['SearchTerm'] = keyword2\n",
    "\n",
    "# Concatenate the DataFrames\n",
    "combined_df = pd.concat([df1, df2], ignore_index=True)\n",
    "\n",
    "# Save the combined DataFrame to an Excel file\n",
    "excel_filename = keyword+\"_pubmed_search_results.xlsx\"\n",
    "combined_df.to_excel(excel_filename, index=False)\n",
    "\n",
    "print(f\"Saved combined search results to {excel_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b070d21-ec38-44c8-9119-a0ea88746198",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[H\u001b[2J"
     ]
    }
   ],
   "source": [
    "# Qwen reads abstract and identify knowledge\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Assuming `tokenizer`, `model`, and `device` are already defined and initialized\n",
    "# Example device initialization: device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Function to interact with the LLM API using the new method, now with customizable system prompts\n",
    "def ask_questions(abstract, questions, system_prompts):\n",
    "    responses = []\n",
    "    for question, system_prompt in zip(questions, system_prompts):\n",
    "        # Combine the question and abstract to form the prompt\n",
    "        prompt_text = question + \" \" + str(abstract)\n",
    "        \n",
    "        # Prepare the messages for the new API, using a customizable system prompt\n",
    "        messages = [\n",
    "            {\"role\": \"system\", \"content\": system_prompt},\n",
    "            {\"role\": \"user\", \"content\": prompt_text}\n",
    "        ]\n",
    "        text = tokenizer.apply_chat_template(\n",
    "            messages,\n",
    "            tokenize=False,\n",
    "            add_generation_prompt=True\n",
    "        )\n",
    "        model_inputs = tokenizer([text], return_tensors=\"pt\").to(device)\n",
    "        \n",
    "        # Generate response with the new API\n",
    "        generated_ids = model.generate(\n",
    "            model_inputs.input_ids,\n",
    "            max_new_tokens=5000\n",
    "        )\n",
    "        generated_ids = [\n",
    "            output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "        ]\n",
    "        \n",
    "        # Decode the generated response\n",
    "        response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "        responses.append(response)\n",
    "    return responses\n",
    "\n",
    "# Read the Excel file\n",
    "file_path = excel_filename  # Replace with your file path\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "questions = [\" \"]  # Placeholder question that won't be used\n",
    "\n",
    "system_prompts = [\n",
    "    \"You are specialized for analyzing scientific paper abstracts, Extract entities and causal relationships from scientific paper abstracts. Focus on genes overexpression/deletion, rationale for expression/deletion, products, and metabolites related to expression/deletion. Output in (expression/deletion of gene xxx, consequence 1), (expression/deletion of gene xxx, consequence 2)... format with no additional text.\"\n",
    "]\n",
    "\n",
    "# Process each abstract and store the response\n",
    "total_rows = len(df)\n",
    "for i, row in df.iterrows():\n",
    "    # Clear the console at the beginning of each iteration\n",
    "    os.system('cls' if os.name == 'nt' else 'clear')\n",
    "\n",
    "    # Since we're only asking one question now, directly get the response for the second (index 0) system prompt\n",
    "    response = ask_questions(row['Abstract'], [questions[0]], [system_prompts[0]])[0]\n",
    "\n",
    "    # Store the response in the DataFrame\n",
    "    df.at[i, 'Answer to Question 2'] = response\n",
    "\n",
    "    # Show the response\n",
    "    print(f\"Response for Row {i+1}:\")\n",
    "    print(f\"Answer to Question 2: {response}\")\n",
    "\n",
    "    # Calculate and show the progress percentage\n",
    "    progress = ((i + 1) / total_rows) * 100\n",
    "    print(f\"Progress: {progress:.2f}% completed\")\n",
    "\n",
    "# Save the updated DataFrame back to an Excel file\n",
    "output_file_path = 'updated(Qwen2.5 32b)_'+keyword+'_causal.xlsx'  # Replace with your desired output file path\n",
    "df.to_excel(output_file_path, index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57f9118-ba3c-4612-b2d1-c986b369a064",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# remove repeat words\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "# Load an Excel file\n",
    "df = pd.read_excel(output_file_path, engine='openpyxl')\n",
    "\n",
    "# Fill NaN values in 'Response to New Question' column with zero\n",
    "df['Answer to Question 2'] = df['Answer to Question 2'].fillna(0)\n",
    "\n",
    "# Convert the column values to strings (to ensure compatibility with re.findall)\n",
    "column_values = df['Answer to Question 2'].astype(str).tolist()\n",
    "\n",
    "# Initialize an empty list to hold entities\n",
    "entities = []\n",
    "\n",
    "# Regular expression to match the pattern (entity A, entity B)\n",
    "pattern = r'\\(([^,]+), ([^\\)]+)\\)'\n",
    "\n",
    "# Iterate over each cell in the column\n",
    "for value in column_values:\n",
    "    # Find all matches of the pattern in the cell\n",
    "    matches = re.findall(pattern, value)\n",
    "    # For each match, extend the entities list with the extracted entities\n",
    "    for match in matches:\n",
    "        entities.extend(match)  # This adds both entity A and entity B to the list\n",
    "\n",
    "# Remove duplicates if necessary\n",
    "entities = list(dict.fromkeys(entities))\n",
    "\n",
    "# Join the entities with commas\n",
    "entities_string = ', '.join(entities)\n",
    "\n",
    "print(entities_string)\n",
    "\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer, util\n",
    "\n",
    "# Load your Excel file\n",
    "file_path = output_file_path\n",
    "df = pd.read_excel(file_path, engine='openpyxl')\n",
    "\n",
    "# Assuming you have a list of entities and their embeddings already\n",
    "entities = [entity.strip() for entity in entities_string.split(',')]\n",
    "t2vmodel = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "embeddings = t2vmodel.encode(entities)\n",
    "\n",
    "# Identify similar phrases and store them in a dictionary\n",
    "similar_phrases = {}\n",
    "\n",
    "# Calculate total iterations for progress tracking\n",
    "total_iterations = sum(range(len(entities)))\n",
    "\n",
    "# Initialize a counter to track progress\n",
    "current_iteration = 0\n",
    "\n",
    "for i in range(len(entities)):\n",
    "    for j in range(i + 1, len(entities)):\n",
    "        similarity = util.pytorch_cos_sim(embeddings[i], embeddings[j])\n",
    "        if similarity.item() > 0.8:\n",
    "            # Assuming entities[i] is the first phrase and entities[j] is the similar one\n",
    "            similar_phrases[entities[j]] = entities[i]\n",
    "\n",
    "        # Update the current iteration counter after each inner loop iteration\n",
    "        current_iteration += 1\n",
    "\n",
    "    # Print the percentage completed\n",
    "    percentage_completed = (current_iteration / total_iterations) * 100\n",
    "    print(f\"Progress: {percentage_completed:.2f}%\")\n",
    "\n",
    "# Note: Printing progress in the inner loop might slow down your code execution,\n",
    "# especially if 'entities' is very large. You might want to update the progress\n",
    "# less frequently, for example, only after each completion of the outer loop.\n",
    "\n",
    "\n",
    "# Specify the column you want to modify\n",
    "specific_column = 'Answer to Question 2'\n",
    "\n",
    "# Calculate total iterations for progress tracking (only for the specific column)\n",
    "total_rows = len(df)\n",
    "current_iteration = 0\n",
    "\n",
    "# Iterate through the specific column to substitute similar phrases\n",
    "for index, row in df.iterrows():\n",
    "    current_iteration += 1\n",
    "    # Print progress every 100 rows to avoid performance degradation\n",
    "    if current_iteration % 100 == 0 or current_iteration == total_rows:\n",
    "        progress_percentage = (current_iteration / total_rows) * 100\n",
    "        print(f\"Progress: {progress_percentage:.2f}% complete.\")\n",
    "    \n",
    "    cell_value = str(row[specific_column])\n",
    "    for similar, original in similar_phrases.items():\n",
    "        # Check if the phrase contains 'Yarrowia', if so, skip substitution\n",
    "        if 'Yarrowia' in cell_value or 'Yarrowia' in similar:\n",
    "            continue\n",
    "        if similar in cell_value:\n",
    "            # Substitute similar phrase with the original phrase, ignoring errors if not found\n",
    "            try:\n",
    "                df.at[index, specific_column] = cell_value.replace(similar, original)\n",
    "            except Exception as e:\n",
    "                print(f\"Error substituting phrase: {e}\")\n",
    "                continue\n",
    "\n",
    "# Save the modified Excel file\n",
    "modified_file_path = 'modified_' + file_path\n",
    "df.to_excel(modified_file_path, index=False, engine='openpyxl')\n",
    "\n",
    "print(\"Excel file has been modified and saved as:\", modified_file_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9f5ece9-0a9a-45af-af4d-5583a119824a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4311922f-a6ec-4a92-bd85-bd7540ad0291",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bio_NEKO2",
   "language": "python",
   "name": "bio_neko2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
